# SerialAgent Configuration
# All values shown are defaults — uncomment and modify as needed.

[context]
bootstrap_max_chars = 20000         # Max chars per workspace file before truncation
bootstrap_total_max_chars = 24000   # Max total chars across all injected files
user_facts_max_chars = 4000         # Max chars for USER_FACTS from SerialMemory
skills_index_max_chars = 2000       # Max chars for compact skills index

[serial_memory]
base_url = "http://localhost:4545/mcp"
# api_key = "sm_your_api_key_here"  # Your SerialMemory API key (use env var in production)
transport = "rest"                   # rest | mcp | hybrid

# mcp_endpoint = "stdio://serialmemory-mcp"
# workspace_id = "serial-assistant"  # Set to isolate from VS Code tenant
timeout_ms = 8000
max_retries = 3
default_user_id = "default_user"

[server]
port = 3210
host = "127.0.0.1"

# ── CORS ──────────────────────────────────────────────────────────────
# allowed_origins: list of origins for CORS. Default: localhost only.
# Use ["*"] for permissive (NOT recommended for production).
# [server.cors]
# allowed_origins = ["http://localhost:3000", "https://myapp.com"]

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Admin
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Admin token env var for gated endpoints (ClawHub install/update/uninstall).
# Set SA_ADMIN_TOKEN in your environment to enable these endpoints.
# [admin]
# token_env = "SA_ADMIN_TOKEN"

[workspace]
path = "./workspace"
state_path = "./data/state"

[skills]
path = "./skills"

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Session Management (OpenClaw-aligned)
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Session keys follow the OpenClaw sessionKey model:
#   agent:<agentId>:<channel>:dm:<peerId>   (per_channel_peer default)
#   agent:<agentId>:<channel>:group:<groupId>
#
# dm_scope: main | per_peer | per_channel_peer | per_account_channel_peer
#   "per_channel_peer" is the secure default for multi-user inboxes
#   (prevents cross-user context leakage).
#
# Session state persists in {workspace.state_path}/sessions/

[sessions]
agent_id = "serial-agent"
dm_scope = "per_channel_peer"

# ── Session lifecycle ─────────────────────────────────────────────
# daily_reset_hour: 0–23 (local gateway time), null disables daily reset
# idle_minutes: reset after N minutes of inactivity, null disables
[sessions.lifecycle]
daily_reset_hour = 4
# idle_minutes = 120

# Per-type overrides (keys: "direct", "group", "thread")
# [sessions.lifecycle.reset_by_type.group]
# daily_reset_hour = 6
# idle_minutes = 60

# Per-channel overrides (keys: "discord", "telegram", "whatsapp", …)
# [sessions.lifecycle.reset_by_channel.discord]
# idle_minutes = 180

# ── Identity linking ─────────────────────────────────────────────
# Collapse the same person across channels into one canonical identity.
# Peer IDs should be prefixed: telegram:123, discord:987, whatsapp:+33…
#
# [[sessions.identity_links]]
# canonical = "alice"
# peer_ids = ["telegram:123", "discord:987", "whatsapp:+33612345678"]
#
# [[sessions.identity_links]]
# canonical = "bob"
# peer_ids = ["discord:456", "slack:U12345"]

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Tools (exec / process)
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# exec: spawn commands foreground or auto-background after yield time
# process: manage background sessions (list/poll/log/write/kill/clear/remove)

[tools.exec]
background_ms = 10000          # ms before auto-backgrounding (0 = always foreground)
timeout_sec = 1800             # hard timeout for foreground commands
cleanup_ms = 1800000           # TTL for finished sessions before cleanup (30 min)
max_output_chars = 1000000     # max output chars kept per process session
# pending_max_output_chars = 500000
notify_on_exit = true          # notify when background process exits
# notify_on_exit_empty_success = false

# ── Exec Security ──────────────────────────────────────────────────────
# [tools.exec_security]
# audit_log = true
# denied_patterns = ["rm\\s+-rf\\s+/", "mkfs\\.", "dd\\s+if=.+of=/dev/"]

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Context Pruning (OpenClaw cache-ttl model)
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Trims oversized tool results before sending to the LLM.
# mode: "off" (disabled) or "cache-ttl" (prune stale tool results)
# Only prunes Tool-role messages; never touches user/assistant content.
# Protects tool results for the last N assistant messages.

[pruning]
mode = "off"                   # "off" or "cache-ttl"
# ttl_seconds = 300            # skip pruning if last LLM call was recent
# keep_last_assistants = 3     # protect tool results for N recent assistants
# min_prunable_chars = 50000   # only prune results longer than this

# [pruning.soft_trim]
# max_chars = 4000             # max chars after soft-trim
# head_chars = 1500            # chars to keep from head
# tail_chars = 1500            # chars to keep from tail

# [pruning.hard_clear]
# enabled = true
# placeholder = "[Old tool result content cleared]"

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# LLM Provider System
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# require_provider: if true, abort startup when no LLM providers init.
#   Default false — gateway boots without API keys (dashboard/nodes/
#   sessions still work). Override with SA_REQUIRE_LLM=1 env var.
#   DEPRECATED: prefer startup_policy for finer control.
#
# startup_policy: controls what happens when LLM providers fail to init.
#   "allow_none" (default) — gateway boots without LLM; dashboard/nodes/
#   sessions all work. Init errors surfaced in /v1/models/readiness.
#   "require_one" — abort startup if no providers init (production).
#
# router_mode: "capability" (auto-select based on requirements) or "fixed"
# Each role maps to "provider_id/model_name"

[llm]
require_provider = false
startup_policy = "allow_none"
router_mode = "capability"
default_timeout_ms = 20000
max_retries = 2

# ── Role assignments ──────────────────────────────────────────────────
# Format: "provider_id/model_name"
# The router resolves provider_id to the matching [[llm.providers]] entry.

[llm.roles.planner]
model = "openai/gpt-4o"
require_tools = true
require_streaming = true

[llm.roles.executor]
model = "openai/gpt-4o"
require_tools = true
require_streaming = true
[[llm.roles.executor.fallbacks]]
model = "anthropic/claude-sonnet-4-20250514"
require_tools = true

[llm.roles.summarizer]
model = "openai/gpt-4o-mini"
require_tools = false
require_streaming = false

[llm.roles.embedder]
model = "openai/text-embedding-3-small"
require_tools = false
require_streaming = false

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Provider Registry
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# kind: openai_compat | anthropic | google | openai_codex_oauth
# auth.mode: api_key | query_param | aws_sigv4 | oauth_device | none
# auth.env: environment variable name containing the API key
# auth.key: direct key (prefer auth.env or auth profiles in production)

# ── OpenAI ────────────────────────────────────────────────────────────
[[llm.providers]]
id = "openai"
kind = "openai_compat"
base_url = "https://api.openai.com/v1"
default_model = "gpt-4o"
[llm.providers.auth]
mode = "api_key"
env = "OPENAI_API_KEY"

# ── Anthropic ─────────────────────────────────────────────────────────
[[llm.providers]]
id = "anthropic"
kind = "anthropic"
base_url = "https://api.anthropic.com"
default_model = "claude-sonnet-4-20250514"
[llm.providers.auth]
mode = "api_key"
header = "x-api-key"
prefix = ""
env = "ANTHROPIC_API_KEY"

# ── Google Gemini ─────────────────────────────────────────────────────
[[llm.providers]]
id = "google"
kind = "google"
base_url = "https://generativelanguage.googleapis.com/v1beta"
default_model = "gemini-2.0-flash"
[llm.providers.auth]
mode = "api_key"
env = "GOOGLE_API_KEY"

# ── OpenRouter (OpenAI-compatible) ────────────────────────────────────
# [[llm.providers]]
# id = "openrouter"
# kind = "openai_compat"
# base_url = "https://openrouter.ai/api/v1"
# default_model = "openai/gpt-4o"
# [llm.providers.auth]
# mode = "api_key"
# env = "OPENROUTER_API_KEY"

# ── Together AI ───────────────────────────────────────────────────────
# [[llm.providers]]
# id = "together"
# kind = "openai_compat"
# base_url = "https://api.together.xyz/v1"
# default_model = "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo"
# [llm.providers.auth]
# mode = "api_key"
# env = "TOGETHER_API_KEY"

# ── Chutes ────────────────────────────────────────────────────────────
# [[llm.providers]]
# id = "chutes"
# kind = "openai_compat"
# base_url = "https://llm.chutes.ai/v1"
# default_model = "deepseek-ai/DeepSeek-V3-0324"
# [llm.providers.auth]
# mode = "api_key"
# env = "CHUTES_API_KEY"

# ── xAI (Grok) ───────────────────────────────────────────────────────
# [[llm.providers]]
# id = "xai"
# kind = "openai_compat"
# base_url = "https://api.x.ai/v1"
# default_model = "grok-3"
# [llm.providers.auth]
# mode = "api_key"
# env = "XAI_API_KEY"

# ── Ollama (local) ────────────────────────────────────────────────────
# [[llm.providers]]
# id = "ollama"
# kind = "openai_compat"
# base_url = "http://localhost:11434/v1"
# default_model = "llama3.1:8b"
# [llm.providers.auth]
# mode = "none"

# ── vLLM (local) ──────────────────────────────────────────────────────
# [[llm.providers]]
# id = "vllm"
# kind = "openai_compat"
# base_url = "http://localhost:8000/v1"
# default_model = "meta-llama/Meta-Llama-3.1-70B-Instruct"
# [llm.providers.auth]
# mode = "none"

# ── Qwen (Alibaba) ───────────────────────────────────────────────────
# [[llm.providers]]
# id = "qwen"
# kind = "openai_compat"
# base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
# default_model = "qwen-plus"
# [llm.providers.auth]
# mode = "api_key"
# env = "DASHSCOPE_API_KEY"

# ── MiniMax ───────────────────────────────────────────────────────────
# [[llm.providers]]
# id = "minimax"
# kind = "openai_compat"
# base_url = "https://api.minimax.chat/v1"
# default_model = "abab6.5s-chat"
# [llm.providers.auth]
# mode = "api_key"
# env = "MINIMAX_API_KEY"

# ── Moonshot ──────────────────────────────────────────────────────────
# [[llm.providers]]
# id = "moonshot"
# kind = "openai_compat"
# base_url = "https://api.moonshot.cn/v1"
# default_model = "moonshot-v1-8k"
# [llm.providers.auth]
# mode = "api_key"
# env = "MOONSHOT_API_KEY"

# ── HuggingFace Inference ────────────────────────────────────────────
# [[llm.providers]]
# id = "huggingface"
# kind = "openai_compat"
# base_url = "https://api-inference.huggingface.co/v1"
# default_model = "meta-llama/Meta-Llama-3.1-70B-Instruct"
# [llm.providers.auth]
# mode = "api_key"
# env = "HF_TOKEN"

# ── Venice AI ─────────────────────────────────────────────────────────
# [[llm.providers]]
# id = "venice"
# kind = "openai_compat"
# base_url = "https://api.venice.ai/api/v1"
# default_model = "llama-3.3-70b"
# [llm.providers.auth]
# mode = "api_key"
# env = "VENICE_API_KEY"

# ── LiteLLM proxy ────────────────────────────────────────────────────
# [[llm.providers]]
# id = "litellm"
# kind = "openai_compat"
# base_url = "http://localhost:4000/v1"
# default_model = "gpt-4o"
# [llm.providers.auth]
# mode = "api_key"
# env = "LITELLM_API_KEY"

# ── Cloudflare Workers AI ────────────────────────────────────────────
# [[llm.providers]]
# id = "cloudflare"
# kind = "openai_compat"
# base_url = "https://api.cloudflare.com/client/v4/accounts/{account_id}/ai/v1"
# default_model = "@cf/meta/llama-3.1-70b-instruct"
# [llm.providers.auth]
# mode = "api_key"
# env = "CLOUDFLARE_API_KEY"
