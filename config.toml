[context]
bootstrap_max_chars = 20000
bootstrap_total_max_chars = 24000
user_facts_max_chars = 4000
skills_index_max_chars = 2000

[serial_memory]
base_url = "http://localhost:4545/mcp"
transport = "rest"
timeout_ms = 8000
max_retries = 3
default_user_id = "default_user"

[server]
port = 3210
host = "127.0.0.1"
# API bearer token — generate with: openssl rand -hex 32
# api_token = ""
api_token_env = "SA_API_TOKEN"

# Admin bearer token — generate with: openssl rand -hex 32
# [admin]
# token = ""

[server.cors]
allowed_origins = [
    "http://localhost:*",
    "http://127.0.0.1:*",
]

[workspace]
path = "./workspace"
state_path = "./data/state"

[skills]
path = "./skills"

[llm]
router_mode = "capability"
default_timeout_ms = 20000
max_retries = 2
require_provider = false
startup_policy = "allow_none"

[llm.roles.summarizer]
model = "openai/gpt-4o-mini"
require_tools = false
require_json = false
require_streaming = false
fallbacks = []

[llm.roles.embedder]
model = "openai/text-embedding-3-small"
require_tools = false
require_json = false
require_streaming = false
fallbacks = []

[llm.roles.planner]
model = "openai/gpt-4o"
require_tools = true
require_json = false
require_streaming = true
fallbacks = []

[llm.roles.executor]
model = "openai/gpt-4o"
require_tools = true
require_json = false
require_streaming = true

[[llm.roles.executor.fallbacks]]
model = "anthropic/claude-sonnet-4-20250514"
require_tools = true
require_json = false

[[llm.providers]]
id = "openai"
kind = "openai_compat"
base_url = "https://api.openai.com/v1"
default_model = "gpt-4o"

[llm.providers.auth]
mode = "api_key"
env = "OPENAI_API_KEY"
keys = []

[[llm.providers]]
id = "anthropic"
kind = "anthropic"
base_url = "https://api.anthropic.com"
default_model = "claude-sonnet-4-20250514"

[llm.providers.auth]
mode = "api_key"
header = "x-api-key"
prefix = ""
env = "ANTHROPIC_API_KEY"
keys = []

[[llm.providers]]
id = "google"
kind = "google"
base_url = "https://generativelanguage.googleapis.com/v1beta"
default_model = "gemini-2.0-flash"

[llm.providers.auth]
mode = "api_key"
env = "GOOGLE_API_KEY"
keys = []

[[llm.providers]]
id = "cc"
kind = "openai_compat"
base_url = "http://localhost:3456/v1"
default_model = "claude-sonnet-4"

[llm.providers.auth]
mode = "api_key"
key = "no_key_required"
keys = []

[[llm.providers]]
id = "custom-api-deepseek-com"
kind = "openai_compat"
base_url = "https://api.deepseek.com/v1"
default_model = "deepseek-chat"

[llm.providers.auth]
mode = "api_key"
key = "REDACTED_DEEPSEEK_KEY"
keys = []

[[llm.providers]]
id = "deepseek"
kind = "openai_compat"
base_url = "https://api.deepseek.com/v1"
default_model = "deepseek-chat"

[llm.providers.auth]
mode = "api_key"
key = "REDACTED_DEEPSEEK_KEY"
keys = []

[[llm.providers]]
id = "nvidia"
kind = "openai_compat"
base_url = "https://integrate.api.nvidia.com/v1"
default_model = "nvidia/llama-3.1-nemotron-70b-instruct"

[llm.providers.auth]
mode = "api_key"
key = "NVIDIA_API_KEY"
keys = []

[[llm.providers]]
id = "venice"
kind = "openai_compat"
base_url = "https://api.venice.ai/api/v1"
default_model = "venice-uncensored"

[llm.providers.auth]
mode = "api_key"
key = "REDACTED_VENICE_KEY"
keys = []

[llm.pricing]

[sessions]
agent_id = "claude-agent"
dm_scope = "per_channel_peer"
identity_links = []

[sessions.lifecycle]
daily_reset_hour = 4

[sessions.lifecycle.reset_by_type]

[sessions.lifecycle.reset_by_channel]

[sessions.send_policy]
default = "allow"
deny_groups = true

[sessions.send_policy.channel_overrides]

[tools.exec]
background_ms = 10000
timeout_sec = 1800
cleanup_ms = 1800000
max_output_chars = 1000000
pending_max_output_chars = 500000
notify_on_exit = true
notify_on_exit_empty_success = false

[tools.exec_security]
audit_log = true
denied_patterns = [
    'rm\s+(-[a-zA-Z]*[rR][a-zA-Z]*\s+|--recursive\s+).*(/|~)',
    'rm\s+-rf\s+/',
    'mkfs\.',
    'dd\s+if=.+of=/dev/',
    '\b(shutdown|reboot|poweroff|halt|init\s+[0-6])\b',
    'chmod\s+(-[a-zA-Z]*\s+)*(0?777|a\+rwx)\s+/',
    'chown\s+(-[a-zA-Z]*\s+)*root[:\s]',
    'curl\s+.*\|\s*(ba)?sh',
    'wget\s+.*\|\s*(ba)?sh',
    'curl\s+.*\|\s*python',
    'wget\s+.*\|\s*python',
    ':\(\)\s*\{.*\}',
    "/dev/tcp/",
    'nc\s+(-[a-zA-Z]*\s+)*-e',
    'ncat\s+(-[a-zA-Z]*\s+)*-e',
    '\bfdisk\s+/dev/',
    '\bparted\s+/dev/',
    '\bshred\s+',
]
approval_patterns = []
approval_timeout_sec = 300

[pruning]
mode = "off"
ttl_seconds = 300
keep_last_assistants = 3
min_prunable_chars = 50000
soft_trim_ratio = 0.3
hard_clear_ratio = 0.5

[pruning.soft_trim]
max_chars = 4000
head_chars = 1500
tail_chars = 1500

[pruning.hard_clear]
enabled = true
placeholder = "[Old tool result content cleared]"

[compaction]
auto = true
max_turns = 80
keep_last_turns = 12

[memory_lifecycle]
auto_capture = true
capture_on_compaction = true

[admin]
token_env = "SA_ADMIN_TOKEN"

[mcp]
servers = []

[mcp.presets.browser]
enabled = false

[mcp.presets.filesystem]
enabled = false

[tasks]
max_concurrent = 5

[agents.kimi-agent]
memory_mode = "shared"
compaction_enabled = false

[agents.kimi-agent.tool_policy]
allow = []
deny = []

[agents.kimi-agent.models]

[agents.kimi-agent.limits]
max_depth = 3
max_children_per_turn = 5
max_duration_ms = 30000

[agents.ollama]
memory_mode = "shared"
compaction_enabled = false

[agents.ollama.tool_policy]
allow = []
deny = []

[agents.ollama.models]

[agents.ollama.limits]
max_depth = 3
max_children_per_turn = 5
max_duration_ms = 30000

[agents.main]
memory_mode = "shared"
compaction_enabled = false

[agents.main.tool_policy]
allow = []
deny = []

[agents.main.models]

[agents.main.limits]
max_depth = 3
max_children_per_turn = 5
max_duration_ms = 30000

[agents.claude-agent]
memory_mode = "shared"
compaction_enabled = false

[agents.claude-agent.tool_policy]
allow = []
deny = []

[agents.claude-agent.models]

[agents.claude-agent.limits]
max_depth = 3
max_children_per_turn = 5
max_duration_ms = 30000

[observability]
service_name = "serialagent"
sample_rate = 1.0

[quota.per_agent]
